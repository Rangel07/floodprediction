{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,\\\n",
    "                            recall_score, fbeta_score, precision_recall_curve\n",
    "\n",
    "from hyperopt import hp\n",
    "import hyperopt.pyll\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Pipeline')\n",
    "\n",
    "from ml_utils import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('../../../data/cleandata/Info pluviometricas/Merged Data/repaired.csv', sep = ';')\n",
    "original_df['Data_Hora'] = pd.to_datetime(original_df['Data_Hora'])\n",
    "original_df['Date'] = original_df['Data_Hora'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_cols = list({c.split('_')[0] for c in original_df.columns if '_error' in c})\n",
    "interest_cols.remove('TemperaturaInterna')\n",
    "interest_cols.remove('SensacaoTermica')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Stations - Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in interest_cols:\n",
    "    original_df[c] = (original_df[c+'_0'] + original_df[c+'_1'] +\n",
    "                      original_df[c+'_2'] + original_df[c+'_3'] + original_df[c+'_4'])/5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = original_df[original_df.Data_Hora.dt.year == 2015]\n",
    "\n",
    "fig = go.Figure(layout=dict(template = 'plotly_dark'))\n",
    "\n",
    "for col in ['PontoDeOrvalho', 'Precipitacao', 'UmidadeRelativa', 'TemperaturaDoAr']:    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = df_plot['Data_Hora'],\n",
    "        y = df_plot[col],\n",
    "        name = col,\n",
    "                            )\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_cols += ['Diff_Temp_POrvalho']\n",
    "original_df['Diff_Temp_POrvalho'] = original_df['TemperaturaDoAr'] -  original_df['PontoDeOrvalho']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Has Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "has_rain_treshold = 10\n",
    "precipitacao_sum = original_df.loc[:, ['Date', 'Precipitacao']].groupby('Date').sum()\n",
    "precipitacao_sum.loc[:, 'Rain_Today'] = precipitacao_sum['Precipitacao'] > has_rain_treshold\n",
    "precipitacao_sum.loc[:, 'Rain_Next_Day'] = precipitacao_sum.loc[:, 'Rain_Today'].shift(-1)\n",
    "precipitacao_sum = precipitacao_sum.dropna()\n",
    "\n",
    "precipitacao_sum.index = pd.to_datetime(precipitacao_sum.index, yearfirst=True)\n",
    "precipitacao_sum.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datewise DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df[interest_cols + ['Date' , 'Data_Hora'] ]\n",
    "df = df.set_index('Data_Hora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = df.index.round('D').unique()\n",
    "df_date = pd.DataFrame(precipitacao_sum.index, columns = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df_date.merge(precipitacao_sum.loc[:, ['Rain_Today','Rain_Next_Day']], on = 'Date')\n",
    "df_date = df_date.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sum_date = df[interest_cols + ['Date']].groupby('Date').sum()\n",
    "sum_date.columns = [c + '_sum' for c in sum_date.columns]\n",
    "\n",
    "# median_date = df[interest_cols + ['Date']].groupby('Date').median()\n",
    "# median_date.columns = [c + '_median' for c in median_date.columns]\n",
    "\n",
    "mean_date = df[interest_cols + ['Date']].groupby('Date').mean()\n",
    "mean_date.columns = [c + '_mean' for c in mean_date.columns]\n",
    "\n",
    "min_date = df[interest_cols + ['Date']].groupby('Date').min()\n",
    "min_date.columns = [c + '_min' for c in min_date.columns]\n",
    "\n",
    "# max_date = df[interest_cols + ['Date']].groupby('Date').max()\n",
    "# max_date.columns = [c + '_max' for c in max_date.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = pd.concat([df_date, min_date, sum_date, mean_date], axis = 1)\n",
    "# df_date = pd.concat([df_date, sum_date, mean_date, median_date, min_date, max_date], axis = 1)\n",
    "df_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = [3, 9, 15, 21 ]\n",
    "for selected_hour in hours:\n",
    "\n",
    "    selected_df = df.loc[(df.index.hour == selected_hour ) & (df.index.minute == 0 ), interest_cols ]\n",
    "    selected_df.index = selected_df.index.round('D')\n",
    "    selected_df.columns = [f'{c}_{selected_hour}H' for c in selected_df.columns]\n",
    "    df_date = pd.concat([df_date, selected_df], axis = 1)\n",
    "\n",
    "df_date = df_date.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date['Rain_Next_Day'] = df_date['Rain_Next_Day'].astype(int)\n",
    "df_date['Rain_Today'] = df_date['Rain_Today'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_cols = {'_'.join(c.split('_')[:-1]) for c in df_date.columns if '15H' in c}\n",
    "\n",
    "for c in h_cols:\n",
    "    df_date[f'{c}_diff_15h9'] =  df_date[f'{c}_15H'] - df_date[f'{c}_9H']\n",
    "    df_date[f'{c}_diff_21h3'] =  df_date[f'{c}_21H'] - df_date[f'{c}_3H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hours:\n",
    "    drop_cols = [c for c in df_date.columns if f'{h}H' in c]\n",
    "df_date = df_date.drop(columns =  drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['Rain_Today', 'Rain_Next_Day',\n",
    "#        'DirecaoDoVento_min',\n",
    "#        'VelocidadeDoVento_min',\n",
    "#        'UmidadeRelativa_min',\n",
    "#        'Precipitacao_min',\n",
    "#        'RadiacaoSolar_min',\n",
    "#        'PontoDeOrvalho_min', \n",
    "#        'TemperaturaDoAr_min',\n",
    "       'PressaoAtmosferica_min',\n",
    "#        'Diff_Temp_POrvalho_min',\n",
    "#        'DirecaoDoVento_sum',\n",
    "#        'VelocidadeDoVento_sum', \n",
    "       'UmidadeRelativa_sum',\n",
    "       'Precipitacao_sum',\n",
    "#        'RadiacaoSolar_sum',\n",
    "       'PontoDeOrvalho_sum',\n",
    "       'TemperaturaDoAr_sum',\n",
    "#        'PressaoAtmosferica_sum',\n",
    "#        'Diff_Temp_POrvalho_sum',\n",
    "       'DirecaoDoVento_mean',\n",
    "       'VelocidadeDoVento_mean',\n",
    "       'UmidadeRelativa_mean',\n",
    "       'Precipitacao_mean',\n",
    "       'RadiacaoSolar_mean',\n",
    "       'PontoDeOrvalho_mean',\n",
    "       'TemperaturaDoAr_mean',\n",
    "       'PressaoAtmosferica_mean',\n",
    "       'Diff_Temp_POrvalho_mean',\n",
    "#        'DirecaoDoVento_3H',\n",
    "#        'VelocidadeDoVento_3H',\n",
    "#        'UmidadeRelativa_3H',\n",
    "#        'Precipitacao_3H',\n",
    "#        'RadiacaoSolar_3H',\n",
    "#        'PontoDeOrvalho_3H',\n",
    "#        'TemperaturaDoAr_3H',\n",
    "#        'PressaoAtmosferica_3H',\n",
    "#        'Diff_Temp_POrvalho_3H',\n",
    "#        'DirecaoDoVento_9H',\n",
    "#        'VelocidadeDoVento_9H',\n",
    "#        'UmidadeRelativa_9H',\n",
    "#        'Precipitacao_9H',\n",
    "#        'RadiacaoSolar_9H',\n",
    "#        'PontoDeOrvalho_9H',\n",
    "#        'TemperaturaDoAr_9H',\n",
    "#        'PressaoAtmosferica_9H',\n",
    "#        'Diff_Temp_POrvalho_9H',\n",
    "#        'DirecaoDoVento_15H',\n",
    "#        'VelocidadeDoVento_15H',\n",
    "#        'UmidadeRelativa_15H',\n",
    "#        'Precipitacao_15H',\n",
    "#        'RadiacaoSolar_15H', \n",
    "#        'PontoDeOrvalho_15H',\n",
    "#        'TemperaturaDoAr_15H',\n",
    "#        'PressaoAtmosferica_15H',\n",
    "#        'Diff_Temp_POrvalho_15H',\n",
    "#        'DirecaoDoVento_diff_15h9',\n",
    "#        'DirecaoDoVento_diff_21h3',\n",
    "       'VelocidadeDoVento_diff_15h9',\n",
    "       'VelocidadeDoVento_diff_21h3',\n",
    "       'UmidadeRelativa_diff_15h9',\n",
    "       'UmidadeRelativa_diff_21h3',\n",
    "       'Precipitacao_diff_15h9',\n",
    "       'Precipitacao_diff_21h3',\n",
    "#        'RadiacaoSolar_diff_15h9',\n",
    "#        'RadiacaoSolar_diff_21h3',\n",
    "       'Diff_Temp_POrvalho_diff_15h9',\n",
    "       'Diff_Temp_POrvalho_diff_21h3',\n",
    "       'PontoDeOrvalho_diff_15h9',\n",
    "       'PontoDeOrvalho_diff_21h3',\n",
    "       'TemperaturaDoAr_diff_15h9',\n",
    "       'TemperaturaDoAr_diff_21h3',\n",
    "       'PressaoAtmosferica_diff_15h9',\n",
    "       'PressaoAtmosferica_diff_21h3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df_date.loc[:,selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_season(Row):\n",
    "    \n",
    "    doy = Row.name.timetuple().tm_yday\n",
    "    \n",
    "    fall_start = datetime.strptime('2020-03-20', '%Y-%m-%d' ).timetuple().tm_yday\n",
    "    summer_start = datetime.strptime('2020-06-20', '%Y-%m-%d' ).timetuple().tm_yday\n",
    "    spring_start = datetime.strptime('2020-09-22', '%Y-%m-%d' ).timetuple().tm_yday\n",
    "    spring_end = datetime.strptime('2020-12-21', '%Y-%m-%d' ).timetuple().tm_yday\n",
    "    \n",
    "    fall = range(fall_start, summer_start)\n",
    "    summer = range(summer_start, spring_start)\n",
    "    spring = range(spring_start, spring_end)\n",
    "    \n",
    "    if doy in fall:\n",
    "        season = 1 #'fall'\n",
    "    elif doy in summer:\n",
    "        season = 2 #'winter'\n",
    "    elif doy in spring:\n",
    "        season = 3 #'spring'\n",
    "    else:\n",
    "        season = 0 #'summer' \n",
    "    \n",
    "    return season\n",
    "\n",
    "df_date['season'] =  df_date.apply(get_season, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_means = ['Precipitacao_mean']#, 'RadiacaoSolar_mean', 'TemperaturaDoAr_mean']\n",
    "\n",
    "for s in seasonal_means:\n",
    "    map_ = dict(df_date.groupby('season').mean()['Precipitacao_mean'])\n",
    "    df_date[f'seasonalMean_{s}'] =  df_date['season'].map(map_)\n",
    "\n",
    "df_date = df_date.drop(columns = ['season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.corr()['Rain_Next_Day'].abs().sort_values(ascending = False).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_date.drop(columns = ['Rain_Next_Day']), df_date.Rain_Next_Day.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()#tree_method = 'gpu_hist')\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "clf.fit(X_train, y_train,  eval_metric=[\"logloss\",\"error\", \"auc\", \"map\"], eval_set=eval_set, verbose=False);\n",
    "\n",
    "keys = clf.evals_result()['validation_0'].keys()\n",
    "\n",
    "fig, ax = plt.subplots( 1, len(keys) ,figsize = (7*len(keys),7))\n",
    "ax = ax.ravel()\n",
    "for i, key in enumerate(keys):\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].plot(clf.evals_result()['validation_0'][key], lw = 3)\n",
    "    ax[i].plot(clf.evals_result()['validation_1'][key], lw = 3)\n",
    "plt.show()\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred, ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date.Rain_Next_Day.value_counts()/df_date.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,14))\n",
    "\n",
    "features_imp = dict(zip(X_train.columns, clf.feature_importances_))\n",
    "features_imp = {k: v for k, v in sorted(features_imp.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.barh(list(features_imp.keys()), features_imp.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(X_train.corr())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "colorscale=[[1.0, \"rgb(240, 0, 0)\"],\n",
    "            [0.6, \"rgb(240, 240, 239)\"],\n",
    "            [0.0, 'rgb(240, 240, 240)']]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Heatmap( z = X_train.corr(),\n",
    "                         x = X_train.columns,\n",
    "                         y = X_train.columns, \n",
    "                         colorscale = colorscale))\n",
    "fig.update_layout(width = 700, height = 700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_correlation(df, threshold):\n",
    "    \n",
    "    dataset = df.copy()\n",
    "    \n",
    "    remove_columns = []\n",
    "    \n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (np.abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    remove_columns.append(colname) # deleting the column from the dataset\n",
    "                    \n",
    "                    \n",
    "    return remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = remove_high_correlation(X_train, 0.7)\n",
    "# remove_columns += ['Precipitacao_min']\n",
    "remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sel = X_test.drop(columns = remove_columns)\n",
    "X_train_sel = X_train.drop(columns = remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap( z = X_train_sel.corr(),\n",
    "                         x = X_train_sel.columns,\n",
    "                         y = X_train_sel.columns) )\n",
    "fig.update_layout(width = 700, height = 700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()#tree_method = 'gpu_hist')\n",
    "\n",
    "eval_set = [(X_train_sel, y_train), (X_test_sel, y_test)]\n",
    "\n",
    "clf.fit(X_train_sel, y_train,  eval_metric=[\"logloss\",\"error\", \"auc\", \"map\"],\n",
    "        eval_set=eval_set, verbose=False, early_stopping_rounds=10,);\n",
    "\n",
    "keys = clf.evals_result()['validation_0'].keys()\n",
    "\n",
    "fig, ax = plt.subplots( 1, len(keys) ,figsize = (7*len(keys),7))\n",
    "ax = ax.ravel()\n",
    "for i, key in enumerate(keys):\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].plot(clf.evals_result()['validation_0'][key], lw = 3)\n",
    "    ax[i].plot(clf.evals_result()['validation_1'][key], lw = 3)\n",
    "plt.show()\n",
    "\n",
    "y_pred = clf.predict(X_test_sel)\n",
    "plot_confusion_matrix(y_test, y_pred, ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,9))\n",
    "\n",
    "features_imp = dict(zip(X_train_sel.columns, clf.feature_importances_))\n",
    "features_imp = {k: v for k, v in sorted(features_imp.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.barh(list(features_imp.keys()), features_imp.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_hyperopt = {\n",
    "    'max_depth':scope.int(hp.quniform('max_depth', 5, 30, 1)),\n",
    "    'n_estimators':scope.int(hp.quniform('n_estimators', 5, 1000, 1)),\n",
    "    'min_child_weight':  scope.int(hp.quniform('min_child_weight', 1, 8, 1)),\n",
    "    'reg_lambda':hp.uniform('reg_lambda', 0.01, 500.0),\n",
    "    'reg_alpha':hp.uniform('reg_alpha', 0.01, 500.0),\n",
    "    'colsample_bytree':hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'early_stopping_rounds':  scope.int(hp.quniform('early_stopping_rounds', 1, 20, 1)),\n",
    "                 }\n",
    "\n",
    "def cost_function(params):\n",
    "    \n",
    "    fit_parameters = {}\n",
    "    fit_parameters['early_stopping_rounds'] = params.pop('early_stopping_rounds')\n",
    "\n",
    "    clf = xgb.XGBClassifier(**params,\n",
    "                            objective=\"binary:logistic\",\n",
    "                            random_state=42)\n",
    "\n",
    "    clf.fit(X_train_sel, y_train, eval_set = eval_set, eval_metric=[\"logloss\"], verbose = False,**fit_parameters)\n",
    "    y_pred = clf.predict(X_test_sel)\n",
    "\n",
    "    return {'loss':-fbeta_score(y_test, y_pred, beta=2),'status': STATUS_OK}\n",
    "\n",
    "num_eval = 250\n",
    "eval_set = [(X_train_sel, y_train), (X_test_sel, y_test)]\n",
    "\n",
    "trials = Trials()\n",
    "best_param = fmin(cost_function,\n",
    "                     param_hyperopt,\n",
    "                     algo=tpe.suggest,\n",
    "                     max_evals=num_eval,\n",
    "                     trials=trials,\n",
    "                     rstate=np.random.RandomState(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param['min_child_weight'] = int(best_param['min_child_weight'])\n",
    "best_param['n_estimators'] = int(best_param['n_estimators'])\n",
    "best_param['max_depth'] = int(best_param['max_depth'])\n",
    "best_param['early_stopping_rounds'] = int(best_param['early_stopping_rounds'])\n",
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_param.copy()\n",
    "\n",
    "fit_parameters = {}\n",
    "fit_parameters['early_stopping_rounds'] = params.pop('early_stopping_rounds')\n",
    "\n",
    "clf = xgb.XGBClassifier(**params,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        random_state=42)\n",
    "\n",
    "clf.fit(X_train_sel, y_train, eval_set = eval_set, eval_metric=[\"logloss\"],\n",
    "        verbose = False,**fit_parameters)\n",
    "y_pred = clf.predict(X_test_sel)\n",
    "y_pred_prob = clf.predict_proba(X_test_sel)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, ['0','1'])\n",
    "evaluate = (y_test, y_pred)\n",
    "print('f1_score: ', f1_score(*evaluate))\n",
    "print('Accuracy: ', accuracy_score(*evaluate))\n",
    "print('Precision: ', precision_score(*evaluate))\n",
    "print('Recall: ', recall_score(*evaluate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(y_test, y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_recall = 0.8\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred_prob[:,1])\n",
    "y_pred_threshold = (y_pred_prob[:,1] > threshold[arg_nearest(recall, desired_recall)]).astype(int)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_threshold, ['0','1'])\n",
    "evaluate = (y_test, y_pred_threshold)\n",
    "print('f1_score: ', f1_score(*evaluate))\n",
    "print('Accuracy: ', accuracy_score(*evaluate))\n",
    "print('Precision: ', precision_score(*evaluate))\n",
    "print('Recall: ', recall_score(*evaluate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda7e5673f44381479f842fe1694b809563"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
