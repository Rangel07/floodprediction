{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True # Merged files\n",
    "SAVE_SINGLES = True # Save each individual xls file into csv\n",
    "SAVE_CONCATENATED = True # Save each station as a csv file\n",
    "INCLUDE_MEAN = False # Include mean of all 4 stations on merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import xlrd\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from os import mkdir\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level = logging.INFO, format = '## Clean - %(levelname)s: %(message)s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def create_dir(_path):\n",
    "    if not os.path.exists(_path):\n",
    "        mkdir(_path)\n",
    "        logging.info(f' creating directory: {_path}')\n",
    "\n",
    "def clean(df, file, save = True):\n",
    "    '''\n",
    "    clean - Clean each file and save as a single csv - This results in multiple files; one for each station\n",
    "    '''\n",
    "    df.columns = (list(df.iloc[2].values)) # Get column names\n",
    "    df = df.loc[:, df.columns.notnull()]   # Remove nan columns\n",
    "    df = df[~((df['Data_Hora'] == 'Data_Hora') &\n",
    "              (df['Pressão Atmosférica'] == 'Pressão Atmosférica'))] # Remove all headers\n",
    "\n",
    "    df = df[df.iloc[:,0].str.contains(':', na = False) &\n",
    "            df.iloc[:,0].str.contains('/', na = False)] # Get data rows only\n",
    "\n",
    "    df.insert(0, 'Data','')\n",
    "    df.insert(1, 'Hora','')\n",
    "    df[['Data', 'Hora']] = df['Data_Hora'].str.split(expand = True)\n",
    "    #df.drop('Data_Hora', axis = 1, inplace = True) # Split into 2 columns\n",
    "    \n",
    "    drop_cols = [4, 6, 7, 10, 12, 14, 15, 17, 20, 22 ]\n",
    "    #    drop_cols = [3, 5, 6, 9, 11, 12, 14, 16, 19, 2]\n",
    "    df = df.drop(df.columns[drop_cols],axis=1)\n",
    "\n",
    "    col_names = ['Data', 'Hora', 'Data_Hora',\n",
    "                 'UmidadeRelativa', 'PressaoAtmosferica',\n",
    "                 'TemperaturaDoAr', 'TemperaturaInterna',\n",
    "                 'PontoDeOrvalho', 'SensacaoTermica',\n",
    "                 'RadiacaoSolar', 'DirecaoDoVento',\n",
    "                 'VelocidadeDoVento', 'Precipitacao']\n",
    "    df.columns = col_names   \n",
    "    df['Local'] = os.path.basename(file).split()[0].split('_')[0]\n",
    "\n",
    "    if save:\n",
    "        save_to_file(df, file)\n",
    "    return df\n",
    "\n",
    "def save_to_file(df, file):\n",
    "    save_path = file.replace('rawdata', 'cleandata')\n",
    "    save_path = save_path.replace('.xls', '.csv')\n",
    "    #logging.info('saving to ', save_path, '\\n')\n",
    "    df.to_csv(save_path, sep = ';', index = False)\n",
    "       \n",
    "def concatenate(df_list, name, concat_path ,save = True):\n",
    "    '''\n",
    "    concatenate - For each station turn mutiples files into one\n",
    "    '''\n",
    "    if df_list:\n",
    "        df = pd.concat(df_list, axis = 0)\n",
    "        if save:\n",
    "            save_concat(df, name, concat_path)\n",
    "            return df\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "def save_concat(df, name, path):\n",
    "    file = pjoin(path, name) + '.csv'\n",
    "    print('$$$$ saving:', file)\n",
    "    df.to_csv(file, sep = ';', index = False)\n",
    "    \n",
    "def include_mean(df):\n",
    "    col_names = ['UmidadeRelativa_', 'PressaoAtmosferica_', 'SensacaoTermica_', \n",
    "                 'RadiacaoSolar_', 'DirecaoDoVento_', 'VelocidadeDoVento_', 'Precipitacao_',\n",
    "                 'PontoDeOrvalho_', 'TemperaturaDoAr_', 'TemperaturaInterna_']\n",
    "    \n",
    "    for col_name in col_names:\n",
    "        selected_cols = [col for col in df.columns if col_name in col]\n",
    "        new_name = col_name + 'mean'\n",
    "        df[new_name] = df[selected_cols].mean(axis = 1, skipna = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/felipe/Documents/TCC'\n",
    "path = pjoin(root, 'data/rawdata/Info pluviometricas')\n",
    "files = []\n",
    "directories = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    directories.extend(d)\n",
    "    for file in f:\n",
    "        if '.xls' in file:\n",
    "            files.append(pjoin(r, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dir\n",
    "_path = pjoin(root, \"data/cleandata\")\n",
    "create_dir(_path)\n",
    "\n",
    "_path = pjoin(root, \"data/cleandata/Info pluviometricas\")\n",
    "create_dir(_path)\n",
    "\n",
    "if SAVE_SINGLES:\n",
    "    for directory in directories:\n",
    "        _path = pjoin(root ,\"data/cleandata/Info pluviometricas\", directory)\n",
    "        create_dir(_path)\n",
    "\n",
    "if SAVE_CONCATENATED:\n",
    "    _path = pjoin(root, \"data/cleandata/Info pluviometricas/Concatenated Data/\")\n",
    "    create_dir(_path)\n",
    "    \n",
    "    for directory in directories:\n",
    "        _path = pjoin(root ,\"data/cleandata/Info pluviometricas/Concatenated Data\", directory)\n",
    "        create_dir(_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csvs\n",
    "dic = {directory: [] for directory in directories}\n",
    "\n",
    "#Load cleaned data into dictonary\n",
    "logging.info(f' loading  {len(files)}/90  files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for file in files:\n",
    "    for d in directories:\n",
    "        if d in file:\n",
    "            logging.info(f' {i+1}/{len(files)}')\n",
    "            filename = os.path.basename(file)\n",
    "            wb = xlrd.open_workbook(file, logfile=open(os.devnull, 'w')) # Supress xlrd warnings\n",
    "            df = pd.read_excel(wb)\n",
    "            dic[d].append( clean(df, file, SAVE_SINGLES) )\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanate and save\n",
    "logging.info(' merging files')\n",
    "concatenated = {}\n",
    "for d in directories:\n",
    "    _path = pjoin(root ,\"data/cleandata/Info pluviometricas/Concatenated Data\", d)\n",
    "    concatenated[d] = concatenate(dic[d], d, _path, SAVE_CONCATENATED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "keys = list(concatenated.keys())\n",
    "estacao0 = concatenated[keys[0]].drop(columns=['Data', 'Hora']).drop_duplicates(subset = ['Data_Hora'])\n",
    "estacao1 = concatenated[keys[1]].drop(columns=['Data', 'Hora']).drop_duplicates(subset = ['Data_Hora'])\n",
    "estacao2 = concatenated[keys[2]].drop(columns=['Data', 'Hora']).drop_duplicates(subset = ['Data_Hora'])\n",
    "estacao3 = concatenated[keys[3]].drop(columns=['Data', 'Hora']).drop_duplicates(subset = ['Data_Hora'])\n",
    "estacao4 = concatenated[keys[4]].drop(columns=['Data', 'Hora']).drop_duplicates(subset = ['Data_Hora'])\n",
    "\n",
    "merge1 = estacao0.merge(estacao1, on = 'Data_Hora', how = 'outer', suffixes = ('_0', '_1'))\n",
    "merge2 = estacao2.merge(estacao3, on = 'Data_Hora', how = 'outer', suffixes = ('_2', '_3'))\n",
    "merge3 = merge1.merge(merge2, on = 'Data_Hora', how = 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manualy ad suffixes to estacao4\n",
    "new_cols = []\n",
    "for col in estacao4.columns:\n",
    "    if col != 'Data_Hora':\n",
    "        col = col + '_4'\n",
    "    new_cols.append(col)\n",
    "estacao4.columns = new_cols\n",
    "\n",
    "merged = merge3.merge(estacao4, on = 'Data_Hora', how = 'outer')\n",
    "\n",
    "merged.insert(0, 'Data','')\n",
    "merged.insert(1, 'Hora','')\n",
    "merged[['Data', 'Hora']] = merged['Data_Hora'].str.split(expand = True)\n",
    "\n",
    "# Sort By Data_Hora\n",
    "merged['Data_Hora'] = pd.to_datetime(merged['Data_Hora'])\n",
    "merged = merged.sort_values('Data_Hora').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INCLUDE_MEAN:\n",
    "    merged = include_mean(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    logging.info(' saving files')\n",
    "    save_path = pjoin(root, 'data/cleandata/Info pluviometricas/Merged Data/')\n",
    "    create_dir(save_path)\n",
    "\n",
    "    merged.to_csv( pjoin(save_path,'merged.csv'),\n",
    "                decimal = '.', sep = ';', index = False)\n",
    "\n",
    "logging.info(' done!')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python_defaultSpec_1597707860564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}