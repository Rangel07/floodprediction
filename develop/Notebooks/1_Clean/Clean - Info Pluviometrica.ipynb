{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True # Merged files\n",
    "SAVE_SINGLES = True # Save each individual xls file into csv\n",
    "SAVE_CONCATANATED = True # Save each station as a csv file\n",
    "INCLUDE_MEAN = False # Include mean of all 4 stations on merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import xlrd\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from os import mkdir\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level = logging.INFO, format = '## Clean - %(levelname)s: %(message)s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def create_dir(_path):\n",
    "    if not os.path.exists(_path):\n",
    "        mkdir(_path)\n",
    "        logging.info(f' creating directory: {_path}')\n",
    "\n",
    "def clean(df, file, save = True):\n",
    "    '''\n",
    "    clean - Clean each file and save as a single csv - This results in multiple files; one for each station\n",
    "    '''\n",
    "    df.columns = (list(df.iloc[2].values)) # Get column names\n",
    "    df = df.loc[:, df.columns.notnull()]   # Remove nan columns\n",
    "    df = df[~((df['Data / Hora'] == 'Data / Hora') &\n",
    "              (df['Pressão Atmosférica'] == 'Pressão Atmosférica'))] # Remove all headers\n",
    "\n",
    "    df = df[df.iloc[:,0].str.contains(':', na = False) &\n",
    "            df.iloc[:,0].str.contains('/', na = False)] # Get data rows only\n",
    "\n",
    "    df.insert(0, 'Data','')\n",
    "    df.insert(1, 'Hora','')\n",
    "    df[['Data', 'Hora']] = df['Data / Hora'].str.split(expand = True)\n",
    "    #df.drop('Data / Hora', axis = 1, inplace = True) # Split into 2 columns\n",
    "    \n",
    "    drop_cols = [4, 6, 7, 10, 12, 14, 15, 17, 20, 22 ]\n",
    "    #    drop_cols = [3, 5, 6, 9, 11, 12, 14, 16, 19, 2]\n",
    "    df = df.drop(df.columns[drop_cols],axis=1)\n",
    "\n",
    "    col_names = ['Data', 'Hora', 'Data / Hora',\n",
    "                 'UmidadeRelativa', 'PressaoAtmosferica',\n",
    "                 'Temperatura do Ar', 'TemperaturaInterna',\n",
    "                 'PontoDeOrvalho', 'SensacaoTermica',\n",
    "                 'RadiacaoSolar', 'DirecaoDoVento',\n",
    "                 'VelocidadeDoVento', 'Precipitacao']\n",
    "    df.columns = col_names   \n",
    "    df['Local'] = os.path.basename(file).split()[0].split('_')[0]\n",
    "\n",
    "    if save:\n",
    "        save_to_file(df, file)\n",
    "    return df\n",
    "\n",
    "def save_to_file(df, file):\n",
    "    save_path = file.replace('rawdata', 'cleandata')\n",
    "    save_path = save_path.replace('.xls', '.csv')\n",
    "    #logging.info('saving to ', save_path, '\\n')\n",
    "    df.to_csv(save_path, sep = ';', index = False)\n",
    "       \n",
    "def concatanate(df_list, name, concat_path ,save = True):\n",
    "    '''\n",
    "    concatanate - For each station turn mutiples files into one\n",
    "    '''\n",
    "    if df_list:\n",
    "        df = pd.concat(df_list, axis = 0)\n",
    "        if save:\n",
    "            save_concat(df, name, concat_path)\n",
    "            return df\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "def save_concat(df, name, path):\n",
    "    file = pjoin(path, name) + '.csv'\n",
    "    print('$$$$ saving:', file)\n",
    "    df.to_csv(file, sep = ';', index = False)\n",
    "    \n",
    "def include_mean(df):\n",
    "    col_names = ['UmidadeRelativa_', 'PressaoAtmosferica_', 'SensacaoTermica_', \n",
    "                 'RadiacaoSolar_', 'DirecaoDoVento_', 'VelocidadeDoVento_', 'Precipitacao_',\n",
    "                 'PontoDeOrvalho_', 'Temperatura do Ar_', 'TemperaturaInterna_']\n",
    "    \n",
    "    for col_name in col_names:\n",
    "        selected_cols = [col for col in df.columns if col_name in col]\n",
    "        new_name = col_name + 'mean'\n",
    "        df[new_name] = df[selected_cols].mean(axis = 1, skipna = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/felipe/Documents/TCC'\n",
    "path = pjoin(root, 'data/rawdata/Info pluviometricas')\n",
    "files = []\n",
    "directories = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    directories.extend(d)\n",
    "    for file in f:\n",
    "        if '.xls' in file:\n",
    "            files.append(pjoin(r, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Paraiso\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Erasmo Assunção\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Camilopolis\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/RM 9\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Vitória\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Paraiso\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Erasmo Assunção\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Camilopolis\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/RM 9\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Vitória\n"
     ]
    }
   ],
   "source": [
    "# Create dir\n",
    "_path = pjoin(root, \"data/cleandata\")\n",
    "create_dir(_path)\n",
    "\n",
    "_path = pjoin(root, \"data/cleandata/Info pluviometricas\")\n",
    "create_dir(_path)\n",
    "\n",
    "if SAVE_SINGLES:\n",
    "    for directory in directories:\n",
    "        _path = pjoin(root ,\"data/cleandata/Info pluviometricas\", directory)\n",
    "        create_dir(_path)\n",
    "\n",
    "if SAVE_CONCATANATED:\n",
    "    _path = pjoin(root, \"data/cleandata/Info pluviometricas/Concatanated Data/\")\n",
    "    create_dir(_path)\n",
    "    \n",
    "    for directory in directories:\n",
    "        _path = pjoin(root ,\"data/cleandata/Info pluviometricas/Concatanated Data\", directory)\n",
    "        create_dir(_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "## Clean - INFO:  loading  90/90  files\n"
     ]
    }
   ],
   "source": [
    "# Load csvs\n",
    "dic = {directory: [] for directory in directories}\n",
    "\n",
    "#Load cleaned data into dictonary\n",
    "logging.info(f' loading  {len(files)}/90  files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "## Clean - INFO:  1/90\n",
      "## Clean - INFO:  2/90\n",
      "## Clean - INFO:  3/90\n",
      "## Clean - INFO:  4/90\n",
      "## Clean - INFO:  5/90\n",
      "## Clean - INFO:  6/90\n",
      "## Clean - INFO:  7/90\n",
      "## Clean - INFO:  8/90\n",
      "## Clean - INFO:  9/90\n",
      "## Clean - INFO:  10/90\n",
      "## Clean - INFO:  11/90\n",
      "## Clean - INFO:  12/90\n",
      "## Clean - INFO:  13/90\n",
      "## Clean - INFO:  14/90\n",
      "## Clean - INFO:  15/90\n",
      "## Clean - INFO:  16/90\n",
      "## Clean - INFO:  17/90\n",
      "## Clean - INFO:  18/90\n",
      "## Clean - INFO:  19/90\n",
      "## Clean - INFO:  20/90\n",
      "## Clean - INFO:  21/90\n",
      "## Clean - INFO:  22/90\n",
      "## Clean - INFO:  23/90\n",
      "## Clean - INFO:  24/90\n",
      "## Clean - INFO:  25/90\n",
      "## Clean - INFO:  26/90\n",
      "## Clean - INFO:  27/90\n",
      "## Clean - INFO:  28/90\n",
      "## Clean - INFO:  29/90\n",
      "## Clean - INFO:  30/90\n",
      "## Clean - INFO:  31/90\n",
      "## Clean - INFO:  32/90\n",
      "## Clean - INFO:  33/90\n",
      "## Clean - INFO:  34/90\n",
      "## Clean - INFO:  35/90\n",
      "## Clean - INFO:  36/90\n",
      "## Clean - INFO:  37/90\n",
      "## Clean - INFO:  38/90\n",
      "## Clean - INFO:  39/90\n",
      "## Clean - INFO:  40/90\n",
      "## Clean - INFO:  41/90\n",
      "## Clean - INFO:  42/90\n",
      "## Clean - INFO:  43/90\n",
      "## Clean - INFO:  44/90\n",
      "## Clean - INFO:  45/90\n",
      "## Clean - INFO:  46/90\n",
      "## Clean - INFO:  47/90\n",
      "## Clean - INFO:  48/90\n",
      "## Clean - INFO:  49/90\n",
      "## Clean - INFO:  50/90\n",
      "## Clean - INFO:  51/90\n",
      "## Clean - INFO:  52/90\n",
      "## Clean - INFO:  53/90\n",
      "## Clean - INFO:  54/90\n",
      "## Clean - INFO:  55/90\n",
      "## Clean - INFO:  56/90\n",
      "## Clean - INFO:  57/90\n",
      "## Clean - INFO:  58/90\n",
      "## Clean - INFO:  59/90\n",
      "## Clean - INFO:  60/90\n",
      "## Clean - INFO:  61/90\n",
      "## Clean - INFO:  62/90\n",
      "## Clean - INFO:  63/90\n",
      "## Clean - INFO:  64/90\n",
      "## Clean - INFO:  65/90\n",
      "## Clean - INFO:  66/90\n",
      "## Clean - INFO:  67/90\n",
      "## Clean - INFO:  68/90\n",
      "## Clean - INFO:  69/90\n",
      "## Clean - INFO:  70/90\n",
      "## Clean - INFO:  71/90\n",
      "## Clean - INFO:  72/90\n",
      "## Clean - INFO:  73/90\n",
      "## Clean - INFO:  74/90\n",
      "## Clean - INFO:  75/90\n",
      "## Clean - INFO:  76/90\n",
      "## Clean - INFO:  77/90\n",
      "## Clean - INFO:  78/90\n",
      "## Clean - INFO:  79/90\n",
      "## Clean - INFO:  80/90\n",
      "## Clean - INFO:  81/90\n",
      "## Clean - INFO:  82/90\n",
      "## Clean - INFO:  83/90\n",
      "## Clean - INFO:  84/90\n",
      "## Clean - INFO:  85/90\n",
      "## Clean - INFO:  86/90\n",
      "## Clean - INFO:  87/90\n",
      "## Clean - INFO:  88/90\n",
      "## Clean - INFO:  89/90\n",
      "## Clean - INFO:  90/90\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for file in files:\n",
    "    for d in directories:\n",
    "        if d in file:\n",
    "            logging.info(f' {i+1}/{len(files)}')\n",
    "            filename = os.path.basename(file)\n",
    "            wb = xlrd.open_workbook(file, logfile=open(os.devnull, 'w')) # Supress xlrd warnings\n",
    "            df = pd.read_excel(wb)\n",
    "            dic[d].append( clean(df, file, SAVE_SINGLES) )\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "## Clean - INFO:  merging files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$ saving: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Paraiso/Paraiso.csv\n",
      "$$$$ saving: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Erasmo Assunção/Erasmo Assunção.csv\n",
      "$$$$ saving: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Camilopolis/Camilopolis.csv\n",
      "$$$$ saving: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/RM 9/RM 9.csv\n",
      "$$$$ saving: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Concatanated Data/Vitória/Vitória.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate and save\n",
    "logging.info(' merging files')\n",
    "concatanated = {}\n",
    "for d in directories:\n",
    "    _path = pjoin(root ,\"data/cleandata/Info pluviometricas/Concatanated Data\", d)\n",
    "    concatanated[d] = concatanate(dic[d], d, _path, SAVE_CONCATANATED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "keys = list(concatanated.keys())\n",
    "estacao0 = concatanated[keys[0]].copy( deep = True).drop(columns=['Data', 'Hora'])\n",
    "estacao1 = concatanated[keys[1]].copy( deep = True).drop(columns=['Data', 'Hora'])\n",
    "estacao2 = concatanated[keys[2]].copy( deep = True).drop(columns=['Data', 'Hora'])\n",
    "estacao3 = concatanated[keys[3]].copy( deep = True).drop(columns=['Data', 'Hora'])\n",
    "estacao4 = concatanated[keys[4]].copy( deep = True).drop(columns=['Data', 'Hora'])\n",
    "\n",
    "merge1 = estacao0.merge(estacao1, on = 'Data / Hora', how = 'outer', suffixes = ('_0', '_1'))\n",
    "merge2 = estacao2.merge(estacao3, on = 'Data / Hora', how = 'outer', suffixes = ('_2', '_3'))\n",
    "merge3 = merge1.merge(merge2, on = 'Data / Hora', how = 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manualy ad suffixes to estacao4\n",
    "new_cols = []\n",
    "for col in estacao4.columns:\n",
    "    if col != 'Data / Hora':\n",
    "        col = col + '_4'\n",
    "    new_cols.append(col)\n",
    "estacao4.columns = new_cols\n",
    "\n",
    "merged = merge3.merge(estacao4, on = 'Data / Hora', how = 'outer')\n",
    "\n",
    "merged.insert(0, 'Data','')\n",
    "merged.insert(1, 'Hora','')\n",
    "merged[['Data', 'Hora']] = merged['Data / Hora'].str.split(expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INCLUDE_MEAN:\n",
    "    merged = include_mean(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "## Clean - INFO:  saving files\n",
      "## Clean - INFO:  creating directory: /home/felipe/Documents/TCC/data/cleandata/Info pluviometricas/Merged Data/\n",
      "## Clean - INFO:  done!\n"
     ]
    }
   ],
   "source": [
    "if SAVE:\n",
    "    logging.info(' saving files')\n",
    "    save_path = pjoin(root, 'data/cleandata/Info pluviometricas/Merged Data/')\n",
    "    create_dir(save_path)\n",
    "\n",
    "    merged.to_csv( pjoin(save_path,'merged.csv'),\n",
    "                decimal = '.', sep = ';', index = False)\n",
    "\n",
    "logging.info(' done!')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda7e5673f44381479f842fe1694b809563"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
